{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight and bias initialisers \n",
    "\n",
    "In this reading we investigate different ways to initialise weights and biases in the layers of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default weights and biases\n",
    "\n",
    "In the models we have worked with so far, we have not specified the initial values of the weights and biases in each layer of our neural networks.\n",
    "\n",
    "The default values of the weights and biases in TensorFlow depend on the type of layers we are using. \n",
    "\n",
    "For example, in a `Dense` layer, the biases are set to zero (`zeros`) by default, while the weights are set according to `glorot_uniform`, the Glorot uniform initialiser. \n",
    "\n",
    "The Glorot uniform initialiser draws the weights uniformly at random from the closed interval $[-c,c]$, where $$c = \\sqrt{\\frac{6}{n_{input}+n_{output}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $n_{input}$ and $n_{output}$ are the number of inputs to, and outputs from the layer respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising your own weights and biases\n",
    "We often would like to initialise our own weights and biases, and TensorFlow makes this process quite straightforward.\n",
    "\n",
    "When we construct a model in TensorFlow, each layer has optional arguments `kernel_initialiser` and `bias_initialiser`, which are used to set the weights and biases respectively.\n",
    "\n",
    "If a layer has no weights or biases (e.g. it is a max pooling layer), then trying to set either `kernel_initialiser` or `bias_initialiser` will throw an error.\n",
    "\n",
    "Let's see an example, which uses some of the different initialisations available in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPooling1D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a model\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(filters=16, kernel_size=3, input_shape=(128, 64), kernel_initializer='random_uniform', bias_initializer=\"zeros\", activation='relu'),\n",
    "    MaxPooling1D(pool_size=4),\n",
    "    Flatten(),\n",
    "    Dense(64, kernel_initializer='he_uniform', bias_initializer='ones', activation='relu'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the following example illustrates, we can also instantiate initialisers in a slightly different manner, allowing us to set optional arguments of the initialisation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some layers to our model\n",
    "\n",
    "model.add(Dense(64, \n",
    "                kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05), \n",
    "                bias_initializer=tf.keras.initializers.Constant(value=0.4), \n",
    "                activation='relu'),)\n",
    "\n",
    "model.add(Dense(8, \n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(gain=1.0, seed=None), \n",
    "                bias_initializer=tf.keras.initializers.Constant(value=0.4), \n",
    "                activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom weight and bias initialisers\n",
    "It is also possible to define your own weight and bias initialisers.\n",
    "Initializers must take in two arguments, the `shape` of the tensor to be initialised, and its `dtype`.\n",
    "\n",
    "Here is a small example, which also shows how you can use your custom initializer in a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom initializer\n",
    "\n",
    "def my_init(shape, dtype=None):\n",
    "    return K.random_normal(shape, dtype=dtype)\n",
    "\n",
    "model.add(Dense(64, kernel_initializer=my_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the summary of our finalised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the initialised weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see the effect of our initialisers on the weights and biases by plotting histograms of the resulting values. Compare these plots with the selected initialisers for each layer above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of weight and bias values\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(12,16))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "\n",
    "# Filter out the pooling and flatten layers, that don't have any weights\n",
    "weight_layers = [layer for layer in model.layers if len(layer.weights) > 0]\n",
    "\n",
    "for i, layer in enumerate(weight_layers):\n",
    "    for j in [0, 1]:\n",
    "        axes[i, j].hist(layer.weights[j].numpy().flatten(), align='left')\n",
    "        axes[i, j].set_title(layer.weights[j].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading and resources \n",
    "* https://keras.io/initializers/\n",
    "* https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
